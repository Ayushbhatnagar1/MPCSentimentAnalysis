{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import csv\n",
    "import syllapy\n",
    "import string\n",
    "\n",
    "# Define the path to the folder containing the PDF files\n",
    "folder_path = r'C:\\Users\\Ayush Bhatnagar\\Desktop\\Plaksha\\sem4\\MLPR\\moms'\n",
    "\n",
    "# Initialize data storage\n",
    "data = []\n",
    "\n",
    "# Function to remove Hindi characters from the text\n",
    "def remove_hindi(text):\n",
    "    return ' '.join(word for word in text.split() if not re.search('[\\u0900-\\u097F]', word))\n",
    "\n",
    "# Load sentiment dictionaries\n",
    "def load_lm_dictionary(lm_path):\n",
    "    lm_dict = {}\n",
    "    with open(lm_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            lm_dict[row['Word'].lower()] = {'positive': int(row.get('Positive', 0)),\n",
    "                                             'negative': int(row.get('Negative', 0))}\n",
    "    return lm_dict\n",
    "\n",
    "def load_correa_dictionary(correa_path):\n",
    "    correa_df = pd.read_excel(correa_path)\n",
    "    correa_dict = {}\n",
    "    for index, row in correa_df.iterrows():\n",
    "        word = str(row['Word']).lower().strip()\n",
    "        positive_score = row['Positive'] if not pd.isna(row['Positive']) else 0\n",
    "        negative_score = row['Negative'] if not pd.isna(row['Negative']) else 0\n",
    "        correa_dict[word] = {'positive': int(positive_score),\n",
    "                             'negative': int(negative_score)}\n",
    "    return correa_dict\n",
    "\n",
    "def load_neutral_dictionary(neutral_path):\n",
    "    neutral_df = pd.read_excel(neutral_path)\n",
    "    neutral_dict = {row['Word'].lower(): 'neutral' for index, row in neutral_df.iterrows()}\n",
    "    return neutral_dict\n",
    "\n",
    "def combine_dictionaries(*dicts):\n",
    "    combined_dict = {}\n",
    "    for d in dicts:\n",
    "        for word, scores in d.items():\n",
    "            if word not in combined_dict:\n",
    "                combined_dict[word] = scores\n",
    "            else:\n",
    "                combined_dict[word]['positive'] += scores.get('positive', 0)\n",
    "                combined_dict[word]['negative'] += scores.get('negative', 0)\n",
    "    return combined_dict\n",
    "\n",
    "lm_dictionary = load_lm_dictionary('lm.csv')\n",
    "correa_dictionary = load_correa_dictionary('correa1.xlsx')\n",
    "neutral_dictionary = load_neutral_dictionary('neutral.xlsx')\n",
    "\n",
    "combined_dictionary = combine_dictionaries(lm_dictionary, correa_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document(document):\n",
    "    words = nltk.word_tokenize(document.lower())\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "    neutral_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in combined_dictionary:\n",
    "            if combined_dictionary[word].get('positive', 0) > 0:\n",
    "                positive_words.append(word)\n",
    "            if combined_dictionary[word].get('negative', 0) > 0:\n",
    "                negative_words.append(word)\n",
    "        if word in neutral_dictionary:\n",
    "            neutral_words.append(word)\n",
    "\n",
    "    positive_score = len(positive_words)\n",
    "    negative_score = len(negative_words)\n",
    "    neutral_count = len(neutral_words)\n",
    "\n",
    "    total_words = len(words)\n",
    "    sentiment_score = (positive_score - negative_score) / total_words if total_words > 0 else 0\n",
    "    neutral_proportion = neutral_count / total_words if total_words > 0 else 0\n",
    "\n",
    "    return sentiment_score, positive_score, negative_score, neutral_proportion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_readability_score(text):\n",
    "    num_monosyllabic_words = count_monosyllabic_words(text)\n",
    "    num_words = count_words(text)\n",
    "    num_sentences = count_sentences(text)\n",
    "    FJP = 1.599 * (num_monosyllabic_words / 100) - 1.015 * (num_words / num_sentences) - 31.517\n",
    "    return FJP\n",
    "\n",
    "# Function to count monosyllabic words\n",
    "def count_monosyllabic_words(text):\n",
    "    words = text.split()\n",
    "    monosyllabic_count = sum(1 for word in words if syllapy.count(word) == 1)\n",
    "    return monosyllabic_count\n",
    "\n",
    "# Function to count words\n",
    "def count_words(text):\n",
    "    words = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return len(words)\n",
    "\n",
    "# Function to count sentences\n",
    "def count_sentences(text):\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Assuming 'folder_path' contains the path to the folder with your PDF files\n",
    "folder_path = r'C:\\Users\\Ayush Bhatnagar\\Desktop\\Plaksha\\sem4\\MLPR\\moms'\n",
    "\n",
    "# Initialize data storage\n",
    "data = []\n",
    "documents = []\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        date = re.search(r'(\\d{2}\\d{2}\\d{2})\\.pdf', file_name)\n",
    "        if date:\n",
    "            date_str = date.group(1)\n",
    "\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''.join(page.extract_text() for page in reader.pages)\n",
    "        text = remove_hindi(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "        filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "        cleaned_text = ' '.join(filtered_words)\n",
    "        documents.append(cleaned_text)\n",
    "        # Assuming sentiment scoring and readability scoring functions are defined elsewhere\n",
    "        sentiment_score, positive_score, negative_score, neutral_proportion = score_document(cleaned_text)\n",
    "        readability_score = calculate_readability_score(cleaned_text)\n",
    "        data.append((date_str, cleaned_text, sentiment_score, positive_score, negative_score, neutral_proportion, readability_score))\n",
    "\n",
    "# TaggedDocument for Doc2Vec\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "\n",
    "# Training the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=50, window=2, min_count=1, workers=4, epochs=100)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data, columns=['Date', 'Cleaned_Text', 'Sentiment_Score', 'Positive_Score', 'Negative_Score', 'Neutral_Proportion', 'Readability_Score'])\n",
    "\n",
    "# Infer vectors for the cleaned text of each document in the DataFrame\n",
    "doc_vectors = [model.infer_vector(word_tokenize(doc.lower())) for doc in df['Cleaned_Text']]\n",
    "\n",
    "# Create a DataFrame from the document vectors\n",
    "doc_vectors_df = pd.DataFrame(doc_vectors)\n",
    "\n",
    "# Concatenate the original data with the Doc2Vec DataFrame along the columns\n",
    "combined_data = pd.concat([df.reset_index(drop=True), doc_vectors_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save the combined data to Excel\n",
    "output_path2 = r'C:\\Users\\Ayush Bhatnagar\\Desktop\\Plaksha\\sem4\\MLPR\\moms\\rbi_mpc_minutes.xlsx'\n",
    "\n",
    "output_path1 = r'C:\\Users\\Ayush Bhatnagar\\Desktop\\Plaksha\\sem4\\MLPR\\moms\\doc2vec.xlsx'\n",
    "doc_vectors_df.to_excel(output_path1, index=False, engine='xlsxwriter')\n",
    "combined_data.to_excel(output_path2, index=False, engine='xlsxwriter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['Cleaned_Text'].tolist()  # Now using df, which we know is correct\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words=stopwords.words('english'), ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Concatenate the original data with the TF-IDF DataFrame along the columns\n",
    "combined_data = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = r'C:\\Users\\Ayush Bhatnagar\\Desktop\\Plaksha\\sem4\\MLPR\\moms\\rbi_mpc_minutes.xlsx'\n",
    "combined_data.to_excel(output_path, index=False, engine='xlsxwriter')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
